{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31b112e-94e6-4212-a998-8f113eb5afcb",
   "metadata": {},
   "source": [
    "# Глубокое машинное обучение "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df2996-529d-4d7b-ad07-07d5f6f0fbbc",
   "metadata": {},
   "source": [
    "## HW1: Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8c5cb-d6b8-46eb-a712-0135d82bde2e",
   "metadata": {},
   "source": [
    "Дата *мягкого* дедлайна: `22.10.2025 23:59`\n",
    "\n",
    "Дата **жесткого** дедлйна: `29.10.2025 23:59`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee79b55-ec93-44da-b76b-e317ca5fb4dd",
   "metadata": {},
   "source": [
    "### **Правила сдачи (обязательно к прочтению)**\n",
    "- После **жесткого** дедлайна работы не принимаются.\n",
    "- После мягкого дедлайна каждый день снимается 10 баллов.\n",
    "- Все похожие друг на друга работы оцениваются в 0.\n",
    "- Если вы использовали какой-то публичный код, то это необходимо указать в комментарии и приложить ссылку на источник, иначе тоже может быть 0. :(\n",
    "- За нечитаемый код, неэффективную реализацию и _некрасивые_ визуализации могут быть сняты баллы.\n",
    "- Все вычисления должны быть воспроизводимые, поэтому не забывайте про `seed=42`, а также не забудьте перезапустить ноутбук и прожать все ячейки - ошибок быть не должно.\n",
    "- Готовый ноутбук со всеми данными необходимо положить в архив. В архив необходимо полдожить ваш requirments.txt со всеми версиями библиотек.\n",
    "- Архив называется следующим образом: `HW1_<your_latin_name>.zip`. Пример: `HW1_malkov.zip`\n",
    "- Для всех трейнов важно предоставить графики лосов/эпох. Легче всего оставить ссылку на wandb, если такой возможности нет, то делайте графики сами.\n",
    "- Использовать LLM можно, но допускается не более 50% сгенерированного кода в каждом задании. В случае использования LLM нужно комментарием указать модель, промт и источник инференса. Если нет комментария или количество сгенерированного кода больше 50%, то за задание ставится 0. Пример:\n",
    "```python\n",
    "# LLM START\n",
    "\"\"\"\n",
    "Model: Qwen3-30B-A3B-FP8 (указать квантизацию и кастомные параметры (top-p, top-p, temperature, ...), если есть)\n",
    "Source: vLLM (здесь может быть API (openrouter.ai), может быть сайт с чатиком (chat.z.ai), может быть тула для локального инференса (lm-studio), может быть TG-бот (@jadvebot))\n",
    "Prompt: Мой китайский друг, представь, что ты Ян Ле Кун, придумай за меня алгоритм детекции тачулечек\n",
    "\"\"\"\n",
    "print(\"Hello world\")\n",
    "# LLM END\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e8ae6-eb71-4215-90fa-130d8bd9cfcc",
   "metadata": {},
   "source": [
    "### **Правила оценки работы**\n",
    "Всего за работу можно получить 50 баллов. Вся домашняя работа делится на Advanced (для сильных) и Regular _(для нормальных)_, каждый из разделов дает по 25 баллов. Только выполнив Advanced задачи, можно получить **отл**. Чтобы получить **хор** (5), достаточно полностью выполнить все Regular задания.\n",
    "Вот как выглядит отображение баллов в оценки:\n",
    "| [От и До) | Оценка |\n",
    "|-----------|--------|\n",
    "| 0-5       | 1      |\n",
    "| 5-10      | 2      |\n",
    "| 10-15     | 3      |\n",
    "| 15-20     | 4      |\n",
    "| 20-26     | 5      |\n",
    "| 26-30     | 6      |\n",
    "| 30-35     | 7      |\n",
    "| 35-40     | 8      |\n",
    "| 40-45     | 9      |\n",
    "| 45-50     | 10     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc6c48-09b8-4d7f-ac0c-56e33fb4979d",
   "metadata": {},
   "source": [
    "### Предисловие\n",
    "Вы уже узнали столько, чтобы попробовать себя в роли настоящего инженера или даже ресерчера. Сегодня мы с нуля пройдем весь путь создания детектора. Наша задача будет заключаться в том, чтобы подготовить модель для детекции медицинских масок на изображении.\\\n",
    "\n",
    "![FaceMask](https://user-images.githubusercontent.com/49322948/159162461-4552eee3-27db-49b2-ab6c-718106adc3c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396de19-1c2b-4250-a915-9d2703e12c78",
   "metadata": {},
   "source": [
    "### 0. Импорт всего, что нужно, и определение всех констант"
   ]
  },
  {
   "cell_type": "code",
   "id": "09e4f198-d284-4086-b652-604dbba80894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:54:20.851390Z",
     "start_time": "2025-10-22T20:54:20.328952Z"
    }
   },
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead, SSDLiteRegressionHead\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 300\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcv2\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrandom\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'cv2'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4b8d1ce4-2c3d-442e-a1ac-dc2dfc37c33a",
   "metadata": {},
   "source": [
    "### (reg, 4.0) 1. Разметка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113332e7-b901-4b86-867a-1d77d1aa8106",
   "metadata": {},
   "source": [
    "Здесь нет ссылки на датасет, потому что в реальной жизни данные вам, скорее всего, никто не даст. Вам нужно самим найти все необходимые данные и преобразовать их к нужному виду.\n",
    "\n",
    "Готовую разметку необходимо подготовить в формате __Pascal VOC__. \n",
    "\n",
    "__Ваш датасет нужно запаковать в архив и отправить вместе с работой!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d0904e-7af2-4a44-8b1c-4e2077c3a538",
   "metadata": {},
   "outputs": [],
   "source": "# https://humansintheloop.org/resources/datasets/medical-mask-dataset/"
  },
  {
   "cell_type": "markdown",
   "id": "00233b7b-f675-4f35-ab3f-98c61148eedc",
   "metadata": {},
   "source": [
    "### 2. Подготовка датасета\n",
    "О, ну вот и задание на код. Всё просто, нужно подготовить `dataset` и `loader`, которые будут использоваться для трейна и теста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0755789-2155-4db0-bf43-18d35b385eda",
   "metadata": {},
   "source": [
    "#### (reg, 1.0) 2.1 Определение датасета.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "975a504e-231a-4955-a530-b1f862c28987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVOCDataset(Dataset):\n",
    "    def __init__(self, *args):\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        return \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # YOUR CODE HERE\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f107d-62f8-49ac-8990-40056e52a243",
   "metadata": {},
   "source": [
    "#### (reg, 1.0) 2.2 Подготовка аугументации\n",
    "Аугументацию необходимо делать с помощью `albumentations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "11757aa4-01b1-496f-9d8d-09a441e1e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "train_transforms = A.Compose([])\n",
    "# Да-да, для теста аугументация не нужна, только базовые преобразования\n",
    "test_transforms = A.Compose([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c302a-f4c1-4485-8ca5-d195852ce4d9",
   "metadata": {},
   "source": [
    "#### 2.3 Cобираем всё вместе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b47990b-e759-4d4b-b20e-f1d7c7373e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# train_dataset =\n",
    "# train_loader = \n",
    "\n",
    "# test_dataset =\n",
    "# test_loader ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491244c2-d8da-40e7-81d0-eff96c97abc6",
   "metadata": {},
   "source": [
    "#### (reg, 0.5) 2.4 Визулизация датасета\n",
    "Визуализируйте пару примеров, чтобы убедиться, что вы нигде не ошиблись в разметке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0564053-a54e-434e-b596-cdd90fdbf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c99d1-44d4-4fdb-b69d-63b607fd8e1a",
   "metadata": {},
   "source": [
    "### 3. Обучение претрейн модели\n",
    "Очень вам завидую, ведь это ваш первый настоящий трейн!\n",
    "\n",
    "В прошлом курсе вы уже обучали бустинги и, может даже, с какими-нибудь сложными `hyperopt` или `gridsearch`, но поверьте — это совсем не то. В этом задании вам пригодится абсолютно всё, что мы к этому времени прошли, + мощности colab/kaggle/... + много времени и терпения. А всё почему? А всё потому, что мы будем с вами реализовывать **SSD** с нуля без каких-либо претрейнов и без VGG в бэкбоне (это всё-таки пережиток прошлого). Ну и сравним, что у нас получится с реализацией из `torchvision`, если всё сойдется."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b906c4-8982-4988-8bb1-bc313d413237",
   "metadata": {},
   "source": [
    "#### 3.0 Теоретическая вводная."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee172fc-0ac1-4cb6-a8fa-dcfe2015f713",
   "metadata": {},
   "source": [
    "Мы уже обсуждали на лекции, что такое SSD, и если кратко, то это One-Stage детектор, аналогичный YOLO, в котором есть несколько трюков:\n",
    "1. Берут готовый бэкбон (VGG16), а не обучают его с нуля на ImageNet.\n",
    "2. Вместо FC слоев используют только свертки. (прям как в RPN Faster-RCNN)\n",
    "3. Из оригинальной VGG убирают все полносвязные слои и добавляют дополнительно 5 слоев сверток.\n",
    "4. Вместо одного последнего слоя/активации (как в YOLO) берут 6 слоев из разных мест сети и «делают на них предсказания». Нужно это для того, чтобы лучше работать с объектами маленьких размеров, с которыми YOLO работает плохо.\n",
    "\n",
    "Если более простыми словами объяснять проблематику из пункта 4, то в случае YOLO мы после всех преобразований оригинального изображения 448х448(x3) мы получаем карту активации 7x7(x1024). Таким образом, для каждого 64x64 квадрата оригинального изображения мы имеем всего 1x1x1024 фичи, характеризующий каждый из квадратов 64x64, при этом пространственной информации о содержании этих квадратов там почти нет. Зато пространственной информации будет больше на более ранних картах активации, например, размера 32х32, где каждый «элемент сетки» уже будет отвечать за квадрат 14x14 оригинального изображения. При этом, очевидно, «фичи» из карты активации 32х32 будут «слабее» фичей из 7х7, просто потому что слой 7х7 глубже. \n",
    "\n",
    "![SSD](https://www.acetrace.app/images/illustrations/ssd.png)\n",
    "\n",
    "Если всё ещё не очень понятно, что в этом вашем SSD происходит, то обязательно посмотрите вот на этот реп: [клик](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e62e0b-742e-4e3d-9861-1eae06a971a6",
   "metadata": {},
   "source": [
    "#### 3.1 Практическая вводная\n",
    "Первое, что вам нужно сделать, — это прочитать [статью](https://arxiv.org/pdf/1512.02325) с вниманием ко всем деталям. Оттуда, например, нужно заимствовать формулки для расчета скейла: $s_k = s_{min} + \\frac{s_{max}-s_{min}}{m-1}(k-1)$ и узнать про _hard negative mining_ (все нюансы из статьи правда важны, её же не глупые люди писали!).\n",
    "\n",
    "Какие-то детали с точки зрения имплементации можно подчерпнуть из:\n",
    "- [официальной имплементации](https://github.com/weiliu89/caffe/blob/2c4e4c2899ad7c3a997afef2c1fbac76adca1bad/examples/ssd/ssd_coco.py).\n",
    "- [популярной учебной имплементации](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/model.py).\n",
    "- [имплементации команды torch](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/ssd.py).\n",
    "- [просто чья-то имплементация](https://github.com/amdegroot/ssd.pytorch/tree/master).\n",
    "\n",
    "Также очень рекомендую прочитать блог команды torch относительно имплементации SSD и SSD-Lite (это SSD с MobileNet в бэкбоне):\n",
    "- [SSD](https://pytorch.org/blog/torchvision-ssd-implementation/).\n",
    "- [SSD-Lite](https://pytorch-hub-preview.netlify.app/blog/torchvision-ssdlite-implementation/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad142a50-316f-4a0f-aef3-1e28154dce90",
   "metadata": {},
   "source": [
    "#### **3.2 План действий (обязательно к прочтению)**\n",
    "Теперь по поводу плана. Если вы не GPU-rich и не имеете доступа к серверу хотя бы с 4 GPU, то у вас вряд ли получится сделать претрейн SSD с нуля, чтобы она прям была пригодна для использования, поэтому мы с вами сфокусируемся на самой архитектуре и на непосредственно процессе обучения. Пункт с обучением своей модели будет выполнен, если:\n",
    "1. Всё в вашей имплементации будет по делу.\n",
    "2. Ваша имплементация будет спокойно **переобучаться** на одном батче за 1000 итераций. Это значит, что вы все правильно имплементировали.\n",
    "3. У вас будет хотя бы **10-20 эпох**, где видно, что лосс классификации и лосс регрессии уверенно сходятся. При этом вам нужно показать минимум **пять** конфигураций обучения (с разными гиперпараметрами/лосами/оптимизациями/архитектурными триками). \n",
    "\n",
    "Если вы все сделали всё правильно, то на ваших графиках loss-а вы увидите что-то такое:\n",
    "<div>\n",
    "<img src=\"https://d3e0luujhwn38u.cloudfront.net/resized/kz9vKVmfahkZKS5UC_tJ8SJVHjeHIGDO8X-wiz8PzU8/s:3200/plain/s3://typefully-user-uploads/img/original/36777/1e59208e-44c4-4ed1-9c13-ee0010afc4cb.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Теперь пару советов с точки зрения самой реализации:\n",
    "1. Бэкбон предлагаю брать максимально компактный типа MobileNetV3.\n",
    "2. Предлагаю не брать какой-то большой датасет типа [COCO](https://public.roboflow.com/object-detection/microsoft-coco-subset/2), а ограничится [PASCAL VOC](https://public.roboflow.com/object-detection/pascal-voc-2012/1).\n",
    "3. Здорово, если вы в какой-то имплементации увидите, что авторы используют SGD, а в другой использует какие трики с масштабированием дисперсии, но будьте уверены, в вашем случае это может не работать или работать не очень хорошо. Нужно все проверять самостоятельно.  \n",
    "\n",
    "Куда обязательно нужно посмотреть: \n",
    "1. Уменьшения изображения — это ок для нашего домашнего претрейна.\n",
    "2. Инициализация весов — это очень важно.\n",
    "3. Сильный дисбаланс классов надо в любом случае компенсировать.\n",
    "\n",
    "И несколько практических советов:\n",
    "1. Обязательно делайте чекпоинты во время трейна.\n",
    "2. Старайтесь сделать код максимально гибким и удобным для ваших экспериментов, а не копипастить одну и ту же функцию трейна с разными оптимизаторами.\n",
    "3. Внимательно смотрите не только на значения лосов по эпохам, но по всем итерациям/степам — это сэкономит вам время.\n",
    "4. Обязательно логируйте все эксперименты в тексте/тензорборде/wandb и смотрите графики. Не забывайте делать оценку на val-выборке, особенно если используете не очень большой датасет типа PASCAL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba7c80-e5c5-4833-a197-6f7bbf981c3c",
   "metadata": {},
   "source": [
    "#### (adv, 5.0) 3.3 Определение модели\n",
    "VGG, скорее всего, очень сильно отличается по архитектуре от выбранного вами бэкбона, поэтому вам самостоятельно нужно подобрать слои, из которых брать фичи, а также определить количество анкеров и базовые соотношения сторон боксов.\n",
    "\n",
    "Для того чтобы долго не высчитывать, какой размер сверток вам нужен на каждом этапе, лучше просто посмотреть, какие шейпы получаются на каждом слое. Вот пример кода:\n",
    "```python\n",
    "backbone = torchvision.models.get_model(\"mobilenet_v3_large\", weights=\"DEFAULT\").features\n",
    "x = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for i, layer in enumerate(backbone):\n",
    "    x = layer(x)\n",
    "    print('back', i, x.shape)\n",
    "\n",
    "extra_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(960, 256, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU6(inplace=True),\n",
    "                SeparableConv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 128, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU6(inplace=True),\n",
    "                SeparableConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "for i, layer in enumerate(extra_layers):\n",
    "    x = layer(x)\n",
    "    print('ext', i, x.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b32fecb-f3c8-4bbd-bdeb-1c0584b37108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySSD(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE \n",
    "\n",
    "    def _generate_default_boxes(self, *args, s_min=0.1, s_max=0.9):\n",
    "        \"\"\"\n",
    "        Функция генерации дефолтных боксов, как из статьи\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24793a-dc36-480c-a97f-83177777b776",
   "metadata": {},
   "source": [
    "#### (adv, 2.0) 3.4 Определение лоса\n",
    "В оригинальной статье лосс состоит из двух компонет: лосс регрессии (L1_smooth) и лосс классификации (кросс-энтропия) с трюком Hard negative mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3f4f70c9-e33e-45ed-be2c-e88d19b82eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySSDLoss(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        # YOUR CODE HERE\n",
    "        return conf_loss + self.alpha * loc_loss, conf_loss, loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0cf28-21cb-40b4-836f-1deb5eb58aa8",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.5 Аугументация для претрейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "020d702f-d7fc-4fd4-be80-478cfec899d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "pre_train_transforms = A.Compose([])\n",
    "pre_val_transforms = A.Compose([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb423f-4f39-4268-a0b7-2983db96cd05",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.6 Подготовка датасета и лоудера для претрейна\n",
    "> Нужно обязательно переисользовать `MyVOCDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4cd3b879-6440-4a84-b7f2-abb98c17e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# pre_train_dataset =\n",
    "# pre_train_loader = \n",
    "\n",
    "# pre_val_dataset =\n",
    "# pre_val_loader =\n",
    "\n",
    "# pre_test_dataset =\n",
    "# pre_test_loader ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8265bc-72a4-4d93-8d9b-98fb2a433f3e",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 3.7 Визулизация датасета\n",
    "Визуализируйте пару примеров, чтобы убедиться, что вы нигде не ошиблись в разметке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a311ff78-5809-43d6-9e39-0e7f64e479d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad76f32-8924-4d73-8278-e978ecc08e23",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.8 Подготовка train-loop для переобученя на одном батче\n",
    "> Не забывайте про перевод модели в режим трнйна (`model.train()`) или в режим инференса (`model.eval()`), а также помните про контекстный менджер: (`torch.no_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d882e35-defc-4a3f-818d-114442678ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_batch_overfit(*args, iterations=1500):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da061e4b-af2e-4ac8-a07e-ad33d38fed0f",
   "metadata": {},
   "source": [
    "#### (adv, 2.0) 3.9 Оверфит на одном батче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6afcf379-3e65-4903-8418-550cae43c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44324dcd-084e-49b6-9302-d12169ce36d4",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.10 Визуализация экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "71e133b3-6928-4058-89c3-a87fc9129ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE OR LINK TO WANDB OR ANY VISUALIZAION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6793646-765b-411f-8a1e-e5439ed4f256",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.11 Подготовка train-loop для обучения претрейна\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "497c694c-64d2-49e0-b731-905b130d83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_stage(*args,):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaef72d-9443-4110-bb5f-c2f42220162c",
   "metadata": {},
   "source": [
    "#### (adv, 8.0) 3.12 Обучение претрейна\n",
    "> Не нужно ждать, что все сойдется, достаточно просто показать 5 разных конфигураций обучения с 20-30 эпохами, где лосс _уверенно_ падает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "176a8f51-5548-4bfd-97eb-3d70d5e25c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e95bc-309b-4fe2-a4c7-0c307ca7f007",
   "metadata": {},
   "source": [
    "#### (adv, 1.0) 3.13 Визуализация экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e403fc77-b107-4df8-88e0-ba1b0c6e4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE OR LINK TO WANDB OR ANY VISUALIZAION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9e9d6-3785-4e37-b7b2-e17b7ea5d022",
   "metadata": {},
   "source": [
    "Если вы дошли до этого пункта, то вы просто герой! („• ֊ •„)੭\n",
    "\n",
    "По секрету: если на собеседовании вы скажете, что вы _Senior AI Developer_, то никто даже не поймет подвоха."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5f92d-1187-4d6c-b2f0-65891847a282",
   "metadata": {},
   "source": [
    "### 4. Обучение основной модели на базе готового претрейна\n",
    "В этом блоке мы зафайнтюним предобученную SSD-Lite на нашей собственной разметке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca4edf9-6d76-4d17-aa91-434392e589de",
   "metadata": {},
   "source": [
    "#### (reg, 1.0) 4.1 Импортируйте веса модели\n",
    "Вот сразу ссылка на сурсы, если что: [клик](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/ssdlite.py). Для вас задача ещё сложнее: дефолтно в [SSD](https://github.com/pytorch/vision/blob/7a13ad0f89167089616b51f4fd07f978cf1f17e4/torchvision/models/detection/ssd.py#L453) и [SSD-Lite](https://github.com/pytorch/vision/blob/7a13ad0f89167089616b51f4fd07f978cf1f17e4/torchvision/models/detection/ssdlite.py#L310) включен NMS, а нам надо его выключить, потому что мы сделаем его сами. Напомнию, что NMS на этапе обучения __[не нужен](https://github.com/pytorch/vision/blob/7a13ad0f89167089616b51f4fd07f978cf1f17e4/torchvision/models/detection/ssd.py#L404)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "47caa8a3-1a0f-4f83-9e1e-f9abed84df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ssdlite320_mobilenet_v3_large(weights=\"DEFAULT\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d1739-0fde-4122-b8b0-ebbfb318b0de",
   "metadata": {},
   "source": [
    "#### (reg, 4.0) 4.1 Подготовьте train-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7b1d28ac-b5cb-4d4a-80f0-d6339319620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssd(*args,):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41155148-5c2f-48a3-8f8a-bbe9aa2b00eb",
   "metadata": {},
   "source": [
    "#### (reg, 3.0) 4.2 Обучите модель на нашем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "beea99e0-ff5b-43c9-a9d9-44a1b86b8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1d111-2592-4338-867d-1a4dd0505fe5",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 4.3 Визуализация эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "86d802f8-dc5d-4e63-97cb-9467f3eeebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE OR LINK TO WANDB OR ANY VISUALIZAION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b5f05-f30c-4feb-84ca-89ac5646ece3",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 4.4 Реализация NMS\n",
    "Мы уже обсуждали алгоритм NMS на лекции, но если забылось, то вот отличная [статья](https://deepmachinelearning.ru/docs/Neural-networks/Object-detection/Non-maximum-supression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3496fa1f-a081-40a0-a2ed-5cfd077376e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(boxes, scores_max, iou_threshold):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf46295-3225-4153-95d9-a36ba4caf869",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 4.5 Реализация функции декодирования предсказаний\n",
    "Нужно реализовать функцию для переовода offset + default_box/anchors, которые выдает модель, в xyxy bbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1cfbaed1-36fe-4de8-96ef-a20086d8e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_boxes(box_preds, default_boxes):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b945e3d-36df-440a-8d92-b948098e40cb",
   "metadata": {},
   "source": [
    "#### (reg, 0.5) 4.5 Визализация работы модели на val датасете\n",
    "Выведит пару примеров детекции масок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "963c7f28-382a-4c24-a914-925b7de682e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346e64a-6ba7-43d1-b49a-ea38075c3d37",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 4.6 Подсчет IoU\n",
    "Для того чтобы оценить качество работы самой детекции чаще всего измеряют IoU. Пропустите вашу модель чере NMS и посчитайте IoU отонсительно Ground Truth. Вычисление IoU нужно реализовать самостоятелньо!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ea2628b5-d9bf-4618-b9d7-d2eddb9f0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxs1, boxs2):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81968fbd-35b9-429f-a82d-b13df4967742",
   "metadata": {},
   "source": [
    "#### (reg, 2.0) 4.7 Подсчет mAP\n",
    "Про `Mean Average Precision` вы точно слышали из прошлого курса. Считается немного модифицированный `PR AUC` по всем наблюдениям одного класса (у нас он только один, фон не считается) и получаем `AP`, а дальше усредняем `AP` по всем классам и получаем `mAP`. [Тут](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) пример на пальцах. Расчет mAP тоже нужно реализовать самостоятельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "076538dd-8108-4617-92c3-b5877aae7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(*args, iou_threshold=0.5, score_threshold=0.01):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88c040-867c-4133-a405-a92316fc69c8",
   "metadata": {},
   "source": [
    "### 5. Обучение модели на базе собственного претрейна\n",
    "**Все пункты дальше необязательные! И их можно делать, только если у вас хоть чуть-чуть сошелся лосс и модель классифицирует объекты лучше рандома!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c26ef-7d48-43c9-8890-4149894a7389",
   "metadata": {},
   "source": [
    "#### 5.1 Дообучение вашего претрейна "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd030fd1-5914-45f7-8b7f-b69335c2e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5dc29-c43b-4218-ada0-e4162dcdb47e",
   "metadata": {},
   "source": [
    "#### 5.2 Визуализация эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c194b66-4744-40f1-b1c5-26cb0108ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb12bf-cede-4209-a193-717f7d72f159",
   "metadata": {},
   "source": [
    "#### 5.3 Подсчет IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c1e85b4e-e6f2-47a9-bb31-02ebdf87e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f315d-7f56-4bf3-9ed0-6dee967a0cd9",
   "metadata": {},
   "source": [
    "#### 5.3 Подсчет mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ca8893c0-f3c4-454a-90d5-6a4126383ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
